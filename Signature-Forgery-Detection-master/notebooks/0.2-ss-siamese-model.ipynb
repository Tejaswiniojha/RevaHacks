{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import layers, Input, Model, optimizers\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/processed/'\n",
    "\n",
    "train = np.load(os.path.join(data_path, 'train.npz'))\n",
    "X_1_train=train['X_1_train']\n",
    "X_2_train=train['X_2_train']\n",
    "X_3_train=train['X_3_train']\n",
    "y_train=train['y_train']\n",
    "\n",
    "valid = np.load(os.path.join(data_path, 'valid.npz'))\n",
    "X_1_valid=valid['X_1_valid']\n",
    "X_2_valid=valid['X_2_valid']\n",
    "X_3_valid=valid['X_3_valid']\n",
    "y_valid=valid['y_valid']\n",
    "\n",
    "test = np.load(os.path.join(data_path, 'test.npz'))\n",
    "X_1_test=test['X_1_test']\n",
    "X_2_test=test['X_2_test']\n",
    "X_3_test=test['X_3_test']\n",
    "y_test=test['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 155, 220, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_1_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 155, 220, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_2_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 155, 220, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_3_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 155, 220, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 145, 210, 96)      11712     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 145, 210, 96)      384       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 72, 104, 96)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 76, 108, 96)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 72, 104, 256)      614656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 72, 104, 256)      1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 35, 51, 256)       0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 35, 51, 256)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPaddin (None, 37, 53, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 35, 51, 384)       885120    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPaddin (None, 37, 53, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 35, 51, 256)       884992    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 17, 25, 256)       0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 17, 25, 256)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 108800)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              111412224 \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               131200    \n",
      "=================================================================\n",
      "Total params: 113,941,312\n",
      "Trainable params: 113,940,608\n",
      "Non-trainable params: 704\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputTensor = Input((155,220,1))\n",
    "\n",
    "conv1 = layers.Conv2D(filters=96, \n",
    "                      kernel_size=(11,11), \n",
    "                      strides=1, \n",
    "                      activation='relu', \n",
    "                      input_shape=(155, 220, 1), \n",
    "                      data_format=\"channels_last\")(inputTensor)\n",
    "\n",
    "conv1_norm = layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001,center=True,\n",
    "            scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "            moving_mean_initializer='zeros',moving_variance_initializer='ones')(conv1)\n",
    "\n",
    "conv1_pool = layers.MaxPooling2D(pool_size=(3,3), \n",
    "                                 strides=2)(conv1_norm)\n",
    "\n",
    "conv2_padding = layers.ZeroPadding2D((2, 2))(conv1_pool)\n",
    "\n",
    "conv2 = layers.Conv2D(filters=256, \n",
    "                      kernel_size=(5,5), \n",
    "                      strides=1, \n",
    "                      activation='relu')(conv2_padding)\n",
    "\n",
    "conv2_norm = layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001,center=True,\n",
    "            scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "            moving_mean_initializer='zeros',moving_variance_initializer='ones')(conv2)\n",
    "\n",
    "conv2_pool = layers.MaxPooling2D(pool_size=(3,3), \n",
    "                                 strides=2)(conv2_norm)\n",
    "\n",
    "conv2_dropout = layers.Dropout(0.3, seed=1)(conv2_pool)\n",
    "\n",
    "conv3_padding = layers.ZeroPadding2D((1,1))(conv2_dropout)\n",
    "\n",
    "conv3 = layers.Conv2D(filters=384, \n",
    "                      kernel_size=(3,3), \n",
    "                      strides=1, \n",
    "                      activation = 'relu')(conv3_padding)\n",
    "\n",
    "conv4_padding = layers.ZeroPadding2D((1,1))(conv3)\n",
    "\n",
    "conv4 = layers.Conv2D(filters=256, \n",
    "                      kernel_size=(3,3), \n",
    "                      strides=1, \n",
    "                      activation='relu')(conv4_padding)\n",
    "                                                                    \n",
    "conv4_pool = layers.MaxPooling2D(pool_size=(3,3), \n",
    "                                 strides=2)(conv4)\n",
    "                                                                    \n",
    "conv4_dropout = layers.Dropout(0.3, seed=1)(conv4_pool)\n",
    "                                                                    \n",
    "flatten_layer = layers.Flatten()(conv4_dropout)\n",
    "                                                                    \n",
    "fully_connected1 = layers.Dense(1024)(flatten_layer)\n",
    "\n",
    "fc1_dropout = layers.Dropout(0.5, seed=1)(fully_connected1)\n",
    "                                                                    \n",
    "embedding = layers.Dense(128)(fc1_dropout)\n",
    "                                                                    \n",
    "embedding_model = Model(inputs=[inputTensor], \n",
    "                         outputs=embedding, \n",
    "                         name='embedding_model')\n",
    "                                                                    \n",
    "embedding_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    :param y_true: TensorFlow/Theano tensor\n",
    "    :param y_pred: TensorFlow/Theano tensor of the same shape as y_true\n",
    "    :return: float\n",
    "    \"\"\"\n",
    "    return K.sqrt(K.sum(K.square(y_pred - y_true), axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(embeddings):\n",
    "    \"\"\"\n",
    "    calculates triplet loss over inputs.\n",
    "    \"\"\"\n",
    "    \n",
    "    processed_a, processed_p, processed_n = embeddings[0], embeddings[1], embeddings[2]\n",
    "    \n",
    "    positive_dist= euclidean_distance_loss(processed_a, processed_p)\n",
    "    negative_dist = euclidean_distance_loss(processed_a, processed_n)\n",
    "       \n",
    "    margin = 0.0\n",
    "    loss = K.maximum(margin, positive_dist - negative_dist)\n",
    "    \n",
    "    return K.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Fake loss function for Keras.\n",
    "    \"\"\"\n",
    "    return y_pred - 0 * y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shivam/.virtualenvs/signature-detection/lib/python2.7/site-packages/ipykernel_launcher.py:39: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"la..., inputs=[<tf.Tenso...)`\n"
     ]
    }
   ],
   "source": [
    "# Siamese model\n",
    "\n",
    "in_dim=(155,220,1)\n",
    "input_anchor = Input(shape=(in_dim))\n",
    "input_positive = Input(shape=(in_dim))\n",
    "input_negative = Input(shape=(in_dim))\n",
    "embedding_a=embedding_model(input_anchor)\n",
    "embedding_p=embedding_model(input_positive)\n",
    "embedding_n=embedding_model(input_negative)\n",
    "\n",
    "\n",
    "# https://github.com/maciejkula/triplet_recommendations_keras/blob/master/triplet_keras.ipynb\n",
    "# https://www.kaggle.com/kmader/image-similarity-with-siamese-networks\n",
    "# https://github.com/keras-team/keras/issues/9498\n",
    "# https://github.com/keras-team/keras/issues/3921#issuecomment-250643688\n",
    "# \n",
    "\n",
    "# NOTE: layers.merge is deprecated. \n",
    "# the only way to do it now is custom keras layer which implements the triplet loss.\n",
    "# Create a loss layer\n",
    "# loss = layers.merge([embedding_a, embedding_p, embedding_n], \n",
    "#                     mode=triplet_loss, \n",
    "#                     name='loss',)\n",
    "\n",
    "\n",
    "# Write a custom layer for loss function.\n",
    "# loss = layers.triplet_loss([embedding_a, embedding_p, embedding_n])\n",
    "\n",
    "embedding_concat = layers.concatenate(inputs=[embedding_a, \n",
    "                                    embedding_p, \n",
    "                                    embedding_n], axis=-1)\n",
    "\n",
    "loss_layer = layers.Lambda(function=triplet_loss, \n",
    "                     output_shape=(1,))\n",
    "\n",
    "loss = loss_layer(embedding_concat)\n",
    "\n",
    "siamese_model = Model(input=[input_anchor, input_positive, input_negative], \n",
    "                      output=loss)\n",
    "\n",
    "siamese_model.compile(loss=identity_loss, optimizer=optimizers.Adam())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 155, 220, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 155, 220, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 155, 220, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_model (Model)         (None, 128)          113941312   input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 384)          0           embedding_model[1][0]            \n",
      "                                                                 embedding_model[2][0]            \n",
      "                                                                 embedding_model[3][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1)            0           concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 113,941,312\n",
      "Trainable params: 113,940,608\n",
      "Non-trainable params: 704\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "siamese_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    }
   ],
   "source": [
    "siamese_model.fit(x=[X_1_train, X_2_train, X_3_train], y=y_train, batch_size=16,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
