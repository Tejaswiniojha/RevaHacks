{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    " - resize images to same dimensions\n",
    " - convert to grayscale\n",
    " - normalize the pixels\n",
    " - split in train-valid-test set and store in data/processed as .npz files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.misc import imresize\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_images = glob.glob('../data/interim/real/*.png')\n",
    "forged_images = glob.glob('../data/interim/forged/*.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_id(image_path):\n",
    "    \"\"\"returns image ID from the image path\"\"\"\n",
    "    image_id = image_path.split('/')[-1].split('_')[0]\n",
    "    return image_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store all images.\n",
    "real_images_dict = defaultdict(list)\n",
    "forged_images_dict = defaultdict(list)\n",
    "\n",
    "# Iterate over real images and put them in dictionary values for same image_id key.\n",
    "for real_image, forged_image in zip(real_images, forged_images):\n",
    "    \n",
    "    # add image to dictionary\n",
    "    real_image_id = get_image_id(real_image)\n",
    "    real_images_dict[real_image_id].append(real_image)\n",
    "    \n",
    "    forged_image_id = get_image_id(forged_image)\n",
    "    forged_images_dict[forged_image_id].append(forged_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tuples of image for training\n",
    "negative_image_tuples = list()\n",
    "# positive_image_tuples = list()\n",
    "\n",
    "for image_id in real_images_dict.keys():\n",
    "    real = real_images_dict[image_id]\n",
    "    forged = forged_images_dict[image_id]\n",
    "    \n",
    "    negative_image_tuples.extend(list(itertools.product(real, real, forged)))\n",
    "#     positive_image_tuples.extend(list(itertools.product(real, real)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(image_path, size=(155, 220)):\n",
    "    \"\"\"returns processed images\"\"\"\n",
    "    # Open image and convert to grayscale.\n",
    "    image = Image.open(image_path)\n",
    "    image = image.convert(\"L\")\n",
    "    \n",
    "    image_array = np.array(image)\n",
    "    \n",
    "    # Resize image to 128, 256 using bilinear interpolation.\n",
    "    image_array_processed = imresize(image_array, size=size, interp='bilinear')\n",
    "    \n",
    "    # Invert pixel values.\n",
    "    image_array_processed = 1 - image_array_processed\n",
    "    \n",
    "    # Normalize by dividing pixel values with standard deviation.\n",
    "    image_array_processed = image_array_processed / np.std(image_array_processed)\n",
    "    \n",
    "    # Expand dimension to (155, 220, 1)\n",
    "    image_array_processed = np.expand_dims(image_array_processed, axis=2)\n",
    "    \n",
    "    return image_array_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4608"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(negative_image_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shivam/.virtualenvs/signature-detection/lib/python2.7/site-packages/ipykernel_launcher.py:10: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "# pre-process data\n",
    "image_1 = []\n",
    "image_2 = []\n",
    "image_3 = []\n",
    "labels = []\n",
    "\n",
    "for anchor, positive, negative in negative_image_tuples[:1000]:\n",
    "    image_1.append(process(anchor))\n",
    "    image_2.append(process(positive))\n",
    "    image_3.append(process(negative))\n",
    "    labels.append(0)\n",
    "    \n",
    "# for real1, real2 in positive_image_tuples:\n",
    "#     image_1.append(process(real1))\n",
    "#     image_2.append(process(real2))\n",
    "#     labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy arrays\n",
    "image_1_array = np.asarray(image_1)\n",
    "image_2_array = np.asarray(image_2)\n",
    "image_3_array = np.asarray(image_3)\n",
    "labels_array = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle numpy arrays\n",
    "idx = np.random.choice(range(len(image_1)), size=len(image_1), replace=False)\n",
    "\n",
    "X_1 = image_1_array[idx]\n",
    "X_2 = image_2_array[idx]\n",
    "X_3 = image_3_array[idx]\n",
    "y = labels_array[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train-valid-test set.\n",
    "train_split = 0.8\n",
    "valid_split = 0.9\n",
    "train_offset = int(train_split * len(X_1))\n",
    "valid_offset = int(valid_split * len(X_1))\n",
    "\n",
    "X_1_train = X_1[:train_offset]\n",
    "X_2_train = X_2[:train_offset]\n",
    "X_3_train = X_3[:train_offset]\n",
    "y_train = y[:train_offset]\n",
    "\n",
    "X_1_valid = X_1[train_offset:valid_offset]\n",
    "X_2_valid = X_2[train_offset:valid_offset]\n",
    "X_3_valid = X_3[train_offset:valid_offset]\n",
    "y_valid = y[train_offset:valid_offset]\n",
    "\n",
    "X_1_test = X_1[valid_offset:]\n",
    "X_2_test = X_2[valid_offset:]\n",
    "X_3_test = X_3[valid_offset:]\n",
    "y_test = y[valid_offset:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((800,), (100,), (100,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, y_valid.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "destn_dir = '../data/processed/'\n",
    "np.savez(os.path.join(destn_dir, 'train.npz'), X_1_train=X_1_train, X_2_train=X_2_train, X_3_train=X_3_train, y_train=y_train)\n",
    "np.savez(os.path.join(destn_dir, 'valid.npz'), X_1_valid=X_1_valid, X_2_valid=X_2_valid, X_3_valid=X_3_valid, y_valid=y_valid)\n",
    "np.savez(os.path.join(destn_dir, 'test.npz'), X_1_test=X_1_test, X_2_test=X_2_test, X_3_test=X_3_test, y_test=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
